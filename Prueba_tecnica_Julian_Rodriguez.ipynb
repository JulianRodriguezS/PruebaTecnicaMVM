{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install boto3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPJR5xUfz_hM",
        "outputId": "2354a8db-1814-47c3-b0a0-740745ca63ce"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting boto3\n",
            "  Downloading boto3-1.34.72-py3-none-any.whl (139 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting botocore<1.35.0,>=1.34.72 (from boto3)\n",
            "  Downloading botocore-1.34.72-py3-none-any.whl (12.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3)\n",
            "  Downloading s3transfer-0.10.1-py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.2/82.2 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.35.0,>=1.34.72->boto3) (2.8.2)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.10/dist-packages (from botocore<1.35.0,>=1.34.72->boto3) (2.0.7)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.72->boto3) (1.16.0)\n",
            "Installing collected packages: jmespath, botocore, s3transfer, boto3\n",
            "Successfully installed boto3-1.34.72 botocore-1.34.72 jmespath-1.0.1 s3transfer-0.10.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DESAFIO #1**: Construya un script que genere de forma automática los datos de: departamentos, puestos de trabajo, y empleados"
      ],
      "metadata": {
        "id": "vR8m-LSOuWTZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QE5bP2h7uU_d",
        "outputId": "ab2bbe59-a864-4122-bb23-0770c3c070fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   departamento_id departamento_nombre\n",
            "0                1              Ventas\n",
            "1                2           Marketing\n",
            "2                3                 I+D\n",
            "3                4    Recursos Humanos\n",
            "4                5        Contabilidad\n",
            "   puesto_id        puesto_nombre\n",
            "0        101              Gerente\n",
            "1        102  Cientifico de datos\n",
            "2        103        Desarrollador\n",
            "3        104            Asistente\n",
            "4        105         Especialista\n",
            "   empleado_id       nombre  departamento_id  puesto_id  salario  \\\n",
            "0         1001   Empleado 1                1        101    57540   \n",
            "1         1002   Empleado 2                3        101    41193   \n",
            "2         1003   Empleado 3                4        101    38344   \n",
            "3         1004   Empleado 4                5        105    78268   \n",
            "4         1005   Empleado 5                1        103    42413   \n",
            "5         1006   Empleado 6                2        105    31984   \n",
            "6         1007   Empleado 7                2        105    20429   \n",
            "7         1008   Empleado 8                5        104    31763   \n",
            "8         1009   Empleado 9                1        105    68922   \n",
            "9         1010  Empleado 10                1        101    65328   \n",
            "\n",
            "  fecha_contratacion  \n",
            "0         2023-12-11  \n",
            "1         2023-06-12  \n",
            "2         2023-09-17  \n",
            "3         2023-11-09  \n",
            "4         2023-08-03  \n",
            "5         2023-09-28  \n",
            "6         2023-11-10  \n",
            "7         2023-07-27  \n",
            "8         2023-01-15  \n",
            "9         2023-10-24  \n"
          ]
        }
      ],
      "source": [
        "# Importamos las librerias necesarias para crear los datos\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "import boto3\n",
        "\n",
        "# Creamos los datos por departamento\n",
        "departamentos_data = {\n",
        "    \"departamento_id\": np.arange(1, 6),\n",
        "    \"departamento_nombre\": [\"Ventas\", \"Marketing\", \"I+D\", \"Recursos Humanos\", \"Contabilidad\"]\n",
        "}\n",
        "\n",
        "# Creamos los datos por puesto de trabajo\n",
        "puestos_data = {\n",
        "    \"puesto_id\": np.arange(101, 106),\n",
        "    \"puesto_nombre\": [\"Gerente\", \"Cientifico de datos\", \"Desarrollador\", \"Asistente\", \"Especialista\"]\n",
        "}\n",
        "\n",
        "# Creamos los datos de empleados teniendo en cuenta los departamentos y los puestos por id\n",
        "def generar_empleados(num_empleados):\n",
        "    empleados_data = {\n",
        "        \"empleado_id\": np.arange(1001, 1001 + num_empleados),\n",
        "        \"nombre\": [f\"Empleado {i}\" for i in range(1, num_empleados + 1)],\n",
        "        \"departamento_id\": np.random.randint(1, 6, size=num_empleados),\n",
        "        \"puesto_id\": np.random.randint(101, 106, size=num_empleados),\n",
        "        \"salario\": np.random.randint(20000, 80000, size=num_empleados),\n",
        "        \"fecha_contratacion\": [datetime(2023, np.random.randint(1, 13), np.random.randint(1, 29)) for _ in range(num_empleados)]\n",
        "    }\n",
        "    return empleados_data\n",
        "\n",
        "# Teniendo en cuenta la funcion generada vamos a crear un total de 10 empleados\n",
        "empleados_data = generar_empleados(10)\n",
        "\n",
        "# almacenamos los datos generados en diferentes dataframes\n",
        "departamentos_df = pd.DataFrame(departamentos_data)\n",
        "puestos_df = pd.DataFrame(puestos_data)\n",
        "empleados_df = pd.DataFrame(empleados_data)\n",
        "\n",
        "print(departamentos_df)\n",
        "print(puestos_df)\n",
        "print(empleados_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DESAFIO #2**: Guarde los datos simulados en archivos con formato CSV/Parquet. Explique el porqué de la escogencia del formato. No descarte usar la\n",
        "capa gratuita de algún servicio de almacenamiento tipo cloud, será considerado un plus\n",
        "\n",
        "**Respuesta:** Se escoje el formato csv ya que es un formato bastante estandar el cual se puede cargar y leer a multiples fuentes de datos, para este caso podemos cargar los archivos csv en blobs de azure o en buckets de s3 de amazon los cuales luego pueden ser procesados y cargados a bases de datos SQL."
      ],
      "metadata": {
        "id": "moH6g1ikwGNR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Exportar los DataFrames a archivos CSV\n",
        "departamentos_df.to_csv('departamentos.csv', index=False)\n",
        "puestos_df.to_csv('puestos.csv', index=False)\n",
        "empleados_df.to_csv('empleados.csv', index=False)"
      ],
      "metadata": {
        "id": "1jvFCLWzwGyy"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DESAFIO #3:** Implemente un proceso batch para migrar los datos a una base de datos SQL/NoSQL, o si lo desea, a un Datawarehouse o bucket analítico\n",
        "de un Datalake. No descarte usar la capa gratuita de algún servicio de almacenamiento tipo cloud, será considerado un plus\n",
        "\n",
        "RESPUESTA: Para este caso vamos a usar un bucket analitico para mantener la carga de los archivos, para este proceso no cuento con el servicio de s3 de AWS pero se dejara el codigo con el cual nos vamos a conectar a este bucket y vamos a realizar el cargue de los archivos a diferentes carpetas dentro del bucket, esto con el fin de definir un proceso en el cual se pueden estar generando multiples archivos y estos se puedan cargar en esta ruta, se deja el codigo con las variables creadas en donde solo tendremos que agregar los datos de la conexion al bucket de s3."
      ],
      "metadata": {
        "id": "gnyQHwv50Py6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuramos los parametros de conexion a nuestro bucket en AWS\n",
        "AWS_ACCESS_KEY_ID = 'TU_ACCESS_KEY_ID' # Access key generado en el bucket para la conexion\n",
        "AWS_SECRET_ACCESS_KEY = 'TU_SECRET_ACCESS_KEY' #secret access key generado en el bucket para la conexion\n",
        "REGION_NAME = 'TU_REGION_NAME'  #'us-east-1' por lo general es una buena region utilizada desde latinoamerica\n",
        "\n",
        "# Realizamos la conexion de la parte del cliente utilizando la libreria instalada previamente la cual es boto3\n",
        "s3_client = boto3.client('s3',\n",
        "                         aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
        "                         aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n",
        "                         region_name=REGION_NAME)\n",
        "\n",
        "# Definimos la ruta donde se van a almacenar los archivos en este caso creamos 3\n",
        "bucket_departamentos = 's3://Proyecto/departamentos/'\n",
        "bucket_puestos = 's3://Proyecto/puestos/'\n",
        "bucket_empleados = 's3://Proyecto/empleados/'\n",
        "\n",
        "# Rutas de los archivos CSV locales\n",
        "departamentos_csv_path = 'departamentos.csv'\n",
        "puestos_csv_path = 'puestos.csv'\n",
        "empleados_csv_path = 'empleados.csv'\n",
        "\n",
        "# Cargar archivos CSV a S3\n",
        "s3_client.upload_file(departamentos_csv_path, bucket_departamentos, 'departamentos.csv')\n",
        "s3_client.upload_file(puestos_csv_path, bucket_puestos, 'puestos.csv')\n",
        "s3_client.upload_file(empleados_csv_path, bucket_empleados, 'empleados.csv')\n",
        "\n",
        "print(\"Archivos CSV cargados exitosamente a Amazon S3.\")"
      ],
      "metadata": {
        "id": "rfJ-D75y0O8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para la visualizacion de los datos desde una base de datos, podemos usar el servicio de Athenas el cual es una base de datos enfocada a temas de datawarehouse en donde podremos visualizar los datos del bucket usando una tabla externa, si no queremos trabajar con la parte de aws podemos hacer el proceso con sql server estableciendo la conexion desde python y realizando el cargue de los archivos."
      ],
      "metadata": {
        "id": "Vu1sksnsP7UW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DESAFIO #4: **Dependiendo si escoge una base de datos SQL/NoSQL, un Datawarehouse, o un Datalake, entonces desarrolle una view/query/report a\n",
        "partir del modelo de datos."
      ],
      "metadata": {
        "id": "bMY4JMnwXGq1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En la parte de la base de datos ya teniendo cargada la informacion se puede crear la siguiente vista la cual nos permite validar cuantas personas se tienen en cada carga, esto es inicial ya que teniendo los datos podemos mejorarla para validar mas aspectos importantes en la vista directamente"
      ],
      "metadata": {
        "id": "Rr7ZzBfFWbxr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Creacion de la vista desde el motor de base de datos\n",
        "create view Personas_por_puesto as\n",
        "select b.puesto_nombre, count(a.puesto_id) as Conteo_Puesto from [dbo].[empleados] a\n",
        "left join [dbo].[puestos] b on a.puesto_id = b.puesto_id\n",
        "group by b.puesto_nombre"
      ],
      "metadata": {
        "id": "Br5bJokDWyGb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}